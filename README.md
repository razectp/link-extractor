# LinkExtractor v1.2.3 ğŸš€

A comprehensive and robust tool for extracting links from websites with multithreading support, intelligent domain filtering, automatic proxy rotation, and **real-time status display**.

## âœ¨ Key Features

### ğŸ¯ **Core Functionality**
- **ğŸ”„ Recursive Crawling**: Automatically follows links found on pages with intelligent termination
- **âš¡ Multithreading**: Configurable number of threads for faster extraction (unlimited)
- **ğŸ¯ Smart Domain Filtering**: Automatically filters to main domain from target, with manual override option
- **ğŸš« Intelligent Domain Ignoring**: Collect ALL links but only crawl non-ignored domains
- **ğŸ­ Random Headers**: Use random user agents to avoid detection and rate limiting
- **ğŸ“Š Progress Tracking**: Real-time progress updates and periodic saves
- **ğŸ›‘ Graceful Interruption**: Stop extraction at any time with Ctrl+C

### ğŸŒ **Advanced Proxy System**
- **ğŸ›¡ï¸ Automatic Proxy Rotation**: Built-in proxy support with automatic failover
- **ğŸ”„ Resilient Connection**: Never gives up on sites, tries multiple proxies automatically
- **âŒ Failed Proxy Tracking**: Remembers and avoids previously failed proxies
- **âœ… Proxy Testing**: Tests each proxy before use to ensure reliability
- **ğŸŒ Country Selection**: Choose specific countries for proxy sources
- **ğŸ”’ Anonymous Proxies**: Only uses anonymous/elite proxies for privacy

### ğŸ¨ **Smart Status Display**
- **âš¡ Real-time Status**: Live updates with activity indicators and metrics
- **ğŸ“Š Zero-Noise Mode**: Completely silent operation, no errors or warnings shown
- **ğŸ§µ Smart Display**: Shows active threads, queue status, proxy info in real-time
- **ğŸ¯ Two Modes**: Clean mode (default) and verbose mode (-v) for debugging

### ğŸ”§ **Resource Management**
- **âš¡ Real-time Saving**: Links saved immediately as they are found (no data loss risk)
- **ğŸ’¾ Continuous Backup**: Automatic periodic saves for additional safety
- **ğŸ§  Memory Safety**: Built-in limits to prevent excessive memory usage
- **ğŸ“ Comprehensive Logging**: Detailed logging with error handling and progress tracking
- **ğŸšï¸ Configurable Limits**: Set maximum links, URLs per domain, and more

## ğŸš€ Installation

Ensure you have Python 3.6+ installed and install the required dependencies:

```bash
pip install -r requirements.txt
```

Or manually install:
```bash
pip install requests beautifulsoup4 lxml free-proxy
```

## ğŸ’» Usage

### ğŸ“‹ **Basic Commands**

```bash
# Simple extraction
python link_extractor.py -d example.com

# With threads and custom output
python link_extractor.py -d example.com -t 25 -o my_links.txt

# With random headers for stealth
python link_extractor.py -d example.com --random-headers

# Domain filtering options
python link_extractor.py -d example.com                              # All domains (default)
python link_extractor.py -d example.com --onlythisdomain             # Auto-filter + subdomains
python link_extractor.py -d example.com --onlythisdomain github.com  # Custom domain + subdomains
```

### ğŸ›¡ï¸ **Proxy Usage**

```bash
# Basic proxy usage (automatic failover)
python link_extractor.py -d example.com --use-proxy

# Specific countries only
python link_extractor.py -d example.com --use-proxy --proxy-countries US GB FR

# Full stealth mode
python link_extractor.py -d example.com --use-proxy --random-headers -t 10
```

### ğŸ¯ **Advanced Options**

```bash
# Ignore specific domains
python link_extractor.py -d example.com --ignore-domains domainstoignore.txt

# Limit extraction
python link_extractor.py -d example.com --max-links 10000

# Complete configuration
python link_extractor.py -d example.com -t 10 -o links.txt --random-headers --use-proxy --proxy-countries US GB --max-links 25000
```

### ğŸ“Š **Status Modes**

```bash
# Clean mode (default) - minimal, real-time status
python link_extractor.py -d example.com --use-proxy

# Verbose mode - detailed debugging logs
python link_extractor.py -d example.com --use-proxy -v
```

## ğŸ“± Status Display Examples

### ğŸ¨ **Clean Mode (Default)**

Perfect for daily use - shows only essential information:

```
ğŸš€ LinkExtractor v1.2.3
ğŸ‘¨â€ğŸ’» Created by Cezar Trainotti Paiva - Pentester and Security Researcher
ğŸ“¡ Target: example.com
ğŸ¯ Domain filter: example.com (auto-detected)
ğŸ’¡ Use -v for detailed logs | Ctrl+C to stop

â ‹ Active | ğŸ”— 127 found | ğŸ“‹ 45 queued | ğŸ§µ 5 working | ğŸ›¡ï¸ proxy123...
â ™ Active | ğŸ”— 284 found | ğŸ“‹ 23 queued | ğŸ§µ 3 working | ğŸ›¡ï¸ proxy456...
âœ… Extraction complete

ğŸ Extraction Complete!
ğŸ“Š Total links found: 284
ğŸ’¾ Saved to: extracted_links.txt
```

**Features:**
- **âš¡ Animated Activity**: Spinner shows real-time activity
- **ğŸ”— Live Counter**: Links found updated in real-time
- **ğŸ“‹ Queue Status**: How many URLs are waiting to be processed
- **ğŸ§µ Active Threads**: Number of working threads
- **ğŸ›¡ï¸ Proxy Info**: Current proxy (when enabled)
- **âŒ Zero Noise**: No errors, warnings, or technical spam

### ğŸ”§ **Verbose Mode (-v flag)**

For debugging and detailed monitoring:

```bash
python link_extractor.py -d example.com -v
```

```
2025-06-09 10:30:15,123 - INFO - Starting recursive link extraction from example.com
2025-06-09 10:30:15,124 - INFO - Using 5 threads
2025-06-09 10:30:15,125 - INFO - Proxy enabled: True
2025-06-09 10:30:15,126 - INFO - Getting new proxy (attempt 1/10)...
2025-06-09 10:30:17,200 - INFO - Successfully configured and tested proxy: http://proxy:8080
2025-06-09 10:30:18,300 - INFO - Processing: https://example.com/
2025-06-09 10:30:19,100 - INFO - Found 25 new links from https://example.com/
```

## ğŸ›ï¸ Command Line Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `-d, --domain` | Target domain to extract links from | **Required** |
| `-t, --threads` | Number of threads to use | 5 |
| `-o, --output` | Output file for extracted links | extracted_links.txt |
| `--random-headers` | Use random user agents in headers | False |
| `--onlythisdomain` | Filter to specific domain. Use without value for auto-detect + subdomains | None (all domains) |
| `--ignore-domains` | File containing domains to ignore | None |
| `--use-proxy` | Enable automatic proxy rotation | False |
| `--proxy-countries` | List of country codes for proxy selection | US GB CA FR DE |
| `--max-links` | Maximum number of links to extract | Unlimited |
| `-v, --verbose` | Enable verbose logging (shows all details) | False |

## ğŸ¯ Smart Domain Filtering (NEW in v1.2.3!)

### ğŸ¤– **Flexible Domain Control**

LinkExtractor now provides intelligent domain filtering with three distinct modes:

**âœ¨ Three Operating Modes:**

1. **ğŸŒ All Domains Mode** (default):
   ```bash
   python link_extractor.py -d example.com
   ```
   - Crawls **ALL domains** found in links
   - No filtering applied
   - Status: `ğŸŒ Domain filter: All domains (no filtering)`

2. **ğŸ¯ Auto-Filter Mode** (use `--onlythisdomain` without value):
   ```bash
   python link_extractor.py -d example.com --onlythisdomain
   ```
   - Auto-extracts main domain from `-d` parameter
   - Includes main domain + all subdomains
   - Example: `example.com` â†’ allows `example.com`, `sub.example.com`, `api.example.com`
   - Status: `ğŸ¯ Domain filter: example.com (includes subdomains)`

3. **âš™ï¸ Custom Filter Mode** (specify domain):
   ```bash
   python link_extractor.py -d example.com --onlythisdomain github.com
   ```
   - Filters to specific domain + subdomains
   - Status: `ğŸ¯ Domain filter: github.com (includes subdomains)`

**ğŸ”§ Auto-Extraction Examples:**
- Input: `example.com` â†’ Filters to: `example.com`
- Input: `www.example.com` â†’ Filters to: `example.com` (removes www)
- Input: `https://sub.example.com/path` â†’ Filters to: `sub.example.com`
- Input: `example.com:8080` â†’ Filters to: `example.com` (removes port)

**ğŸ¯ Subdomain Support:**
When filtering is enabled, the tool allows:
- âœ… `example.com` 
- âœ… `subdomain.example.com`
- âœ… `api.subdomain.example.com`
- âŒ `othersite.com`

## ğŸš« Domain Filtering System

### ğŸ“Š **How Domain Ignoring Works**

The `--ignore-domains` feature provides intelligent link handling:

**âœ… Links ARE Collected (saved to output file):**
- ALL valid links found on pages are collected and saved
- INCLUDING links from ignored domains (Facebook, Google, etc.)
- Complete link discovery for comprehensive analysis

**âŒ Links are NOT Crawled (not processed recursively):**
- Ignored domain links are not visited by the crawler
- No HTTP requests made to ignored domains
- Resource conservation and focused crawling

### ğŸ“ **Example domainstoignore.txt**

```
google.com
facebook.com
instagram.com
twitter.com
linkedin.com
youtube.com
cloudflare.com
googleapis.com
```

### ğŸ’¡ **Practical Example**

**Command:**
```bash
python link_extractor.py -d mysite.com --ignore-domains domainstoignore.txt
```

**Result:**
- âœ… `https://mysite.com/about` â†’ **Will be visited and processed**
- âœ… `https://mysite.com/contact` â†’ **Will be visited and processed**
- ğŸ“Š `https://facebook.com/mysite-page` â†’ **Collected but NOT crawled**
- ğŸ“Š `https://twitter.com/mysite-account` â†’ **Collected but NOT crawled**

## ğŸŒ Proxy System Details

### ğŸ›¡ï¸ **Resilient Proxy Features**

- **ğŸ”„ Smart Rotation**: Changes proxy only when failures occur
- **ğŸ§  Memory**: Remembers and avoids failed proxies
- **âœ… Pre-testing**: Tests each proxy before use
- **ğŸ” Auto-retry**: Multiple attempts to get working proxy
- **ğŸŒ Country Selection**: Choose specific regions
- **ğŸ›‘ Never Gives Up**: Tries multiple proxies until success

### ğŸŒ **Available Countries**

**Recommended Groups:**
- **Fast**: US, GB, CA, DE, NL, FR
- **Reliable**: US, GB, CA, AU, NL, SE, CH  
- **EU**: GB, DE, FR, NL, IT, ES, SE, DK
- **Americas**: US, CA, BR, MX, AR, CL

### âš ï¸ **Proxy Best Practices**

- Start with fewer threads (5-10) when using proxies
- Proxies can be slower than direct connections
- Monitor logs for proxy rotation messages
- Combine with `--random-headers` for better anonymity

## ğŸ“ˆ Resource Limits & Safety

### ğŸ›¡ï¸ **Built-in Safety Features**

- **ğŸ“Š Max URLs per domain**: 1,000 (prevents infinite crawling)
- **ğŸ§  Queue size limit**: 10,000 URLs (prevents memory issues)
- **â±ï¸ Request delays**: Small delays between requests
- **âš¡ Real-time saves**: Every link saved immediately when found
- **ğŸ’¾ Backup saves**: Periodic saves every 5 minutes for additional safety
- **ğŸ”„ Connection pooling**: Efficient resource management

### âš™ï¸ **Configurable Limits**

```bash
# Limit total links extracted
python link_extractor.py -d site.com --max-links 50000

# Control thread count for resource management
python link_extractor.py -d site.com -t 5  # Conservative
python link_extractor.py -d site.com -t 20 # Aggressive
```

## ğŸ“‹ Changelog

### **v1.2.2** - Latest
- **Instant shutdown** - Ctrl+C now terminates immediately (within 1 second)
- **Aggressive signal handling** - Uses signal handlers for faster response
- **Optimized cleanup** - Minimal operations during shutdown for speed

### **v1.2.1**
- **Real-time link saving** - Links saved immediately as they are found
- **Zero data loss** - No risk of losing extracted links on interruption
- **Efficient file handling** - Thread-safe append operations for performance

### **v1.2.0**
- **Real-time status display** with animated progress indicators
- **Complete noise suppression** - urllib3/requests warnings silenced
- **Smart status line** showing threads, queue, links, and proxy info
- **Improved proxy system** with better failover and testing

### **v1.1.1**
- **Resilient proxy system** - never gives up on sites
- **Failed proxy tracking** and automatic avoidance
- **Configurable max links** parameter
- **Unlimited extraction** by default

### **v1.1.0**
- **Automatic proxy rotation** with country selection
- **Proxy failover system** with smart retry logic
- **Enhanced error handling** for proxy failures

### **v1.0.0**
- Initial release with recursive link extraction
- Multithreading support and domain filtering
- Random headers and comprehensive logging

## ğŸ¯ Use Cases

### ğŸ” **Penetration Testing**
```bash
# Comprehensive site mapping with stealth
python link_extractor.py -d target.com --use-proxy --random-headers -t 15
```

### ğŸ“Š **SEO Analysis**
```bash
# Extract all links for analysis
python link_extractor.py -d mysite.com --onlythisdomain mysite.com --max-links 10000
```

### ğŸ•µï¸ **Competitive Research**
```bash
# Extract competitor links while avoiding common domains
python link_extractor.py -d competitor.com --ignore-domains domainstoignore.txt --use-proxy
```

### ğŸ§ª **Development & Testing**
```bash
# Quick extraction with detailed logs
python link_extractor.py -d localhost:3000 -v --max-links 100
```

## âš¡ Performance Tips

1. **Start Conservative**: Begin with 5-10 threads, increase gradually
2. **Use Proxy Wisely**: Enable only when needed (may be slower)
3. **Monitor Resources**: Watch CPU/memory usage with high thread counts
4. **Set Limits**: Use `--max-links` for large sites to prevent runaway extraction
5. **Clean Mode**: Use default clean mode for better terminal experience

## âš ï¸ Legal Notice

This tool is provided as-is for educational and legitimate testing purposes. Use responsibly and in accordance with applicable laws and regulations. Always respect robots.txt files and website terms of service.

---

**ğŸš€ LinkExtractor v1.2.2** - Built for speed, designed for reliability, perfected for usability. 